{"cells":[{"cell_type":"markdown","metadata":{"id":"ymyGX3NCo54W"},"source":["## Setup File for Keras Models\n","Use `%run Setup.ipynb` in another notebook to perform all these tasks automatically.\n","\n","Parameters that can be re-configured:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"CP5v1ZWko54Z","executionInfo":{"status":"error","timestamp":1682362895494,"user_tz":420,"elapsed":5,"user":{"displayName":"Dhanush Jain Mahaveera","userId":"14644500468520180882"}},"outputId":"d243d584-7e4a-4b30-dfb1-21e38801c227"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4ecdf49ffed6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mVALIDATION_SPLIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;31m# embedding dimensions for word vectors (word2vec/GloVe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mGLOVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"dataset/glove/glove.twitter.27B.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m print(\"[i] Loaded Parameters:\\n\",\n\u001b[1;32m      7\u001b[0m       \u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'builtin_function_or_method' and 'str'"]}],"source":["MAX_NB_WORDS = 40000 # max no. of words for tokenizer\n","MAX_SEQUENCE_LENGTH = 30 # max length of text (words) including padding\n","VALIDATION_SPLIT = 0.2\n","EMBEDDING_DIM = 200 # embedding dimensions for word vectors (word2vec/GloVe)\n","GLOVE_DIR = dir+\"dataset/glove/glove.twitter.27B.\"+str(200)+\"d.txt\"\n","print(\"[i] Loaded Parameters:\\n\",\n","      MAX_NB_WORDS,MAX_SEQUENCE_LENGTH+5,\n","      VALIDATION_SPLIT,EMBEDDING_DIM,\"\\n\",\n","      GLOVE_DIR)"]},{"cell_type":"markdown","metadata":{"id":"_n2YJBP5o54b"},"source":["Imports:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0mi4Q-lo54b","outputId":"e5130420-14b4-4469-f1cd-71c8d2d3bf78"},"outputs":[{"name":"stdout","output_type":"stream","text":["[i] Importing Modules...\n"]}],"source":["print(\"[i] Importing Modules...\")\n","import numpy as np\n","import pandas as pd\n","import re, sys, os, csv, keras, pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6zcVTaNo54c","outputId":"8a80c2fd-5493-4842-920a-595cfb2b249a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[+] Using Keras version 2.12.0\n"]}],"source":["from keras import regularizers, initializers, optimizers, callbacks\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","from keras.layers import Embedding\n","from keras.layers import Dense, Input, Flatten, Concatenate\n","from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n","from keras.models import Model\n","from keras import backend as K\n","from keras.layers import Layer, InputSpec\n","print(\"[+] Using Keras version\",keras.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpT9JTWqo54c","outputId":"df4fe079-5448-4d20-c0bd-601dbe77b540"},"outputs":[{"name":"stdout","output_type":"stream","text":["[+] Finished Importing Modules\n"]}],"source":["print(\"[+] Finished Importing Modules\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ti_HOcxqo54d","outputId":"1a418538-9c33-4b08-824e-5b510e24d43c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[i] Reading from csv file...Done!\n"]}],"source":["texts, labels = [], []\n","print(\"[i] Reading from csv file...\", end=\"\")\n","with open(dir+'data.csv') as csvfile:\n","    readCSV = csv.reader(csvfile, delimiter=',')\n","    for row in readCSV:\n","        texts.append(row[0])\n","        labels.append(row[1])\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"WRIPHULco54d"},"source":["Convert text to word tokens (numbers that refer to the words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGk1Ut0Fo54d","outputId":"060da679-88b4-46e9-8bae-2e34ad5d842b"},"outputs":[{"data":{"text/plain":["'\\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\\ntokenizer.fit_on_texts(texts)\\nwith open(\\'tokenizer.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(\"[i] Saved word tokenizer to file: tokenizer.pickle\")\\n'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","print(\"[i] Saved word tokenizer to file: tokenizer.pickle\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVOvEKb3o54e"},"outputs":[],"source":["with open(dir+'tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"]},{"cell_type":"markdown","metadata":{"id":"tr2ulcd6o54e"},"source":["Convert tweets to sequences of word tokens with zero padding at the front and back"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOL4HeaIo54e","outputId":"a77aa859-1985-4c8a-9628-8c91787d1258"},"outputs":[{"name":"stdout","output_type":"stream","text":["[i] Found 34359 unique tokens.\n"]}],"source":["sequences = tokenizer.texts_to_sequences_generator(list(map(lambda x:x.split(),texts)))\n","word_index = tokenizer.word_index\n","print('[i] Found %s unique tokens.' % len(word_index))\n","data_int = pad_sequences(list(sequences), padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-5))\n","data = pad_sequences(data_int, padding='post', maxlen=(MAX_SEQUENCE_LENGTH))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2fLEBqoo54e","outputId":"86b36052-f5b4-4a9c-bd19-dcdfa48a8653"},"outputs":[{"data":{"text/plain":["array([   0,    0,    0,    0,    0,   44,   27,   42,  115,   98, 1637,\n","         98, 3230,   98, 8802,   98, 1070,   98,  194,  219,  178,    5,\n","         31,   32,  445,    0,    0,    0,    0,    0], dtype=int32)"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRqsi5Pso54f","outputId":"392ba6dc-866b-4581-ff7c-01ac74f9f886"},"outputs":[{"name":"stdout","output_type":"stream","text":["[+] Shape of data tensor: (1, 30)\n","[+] Shape of label tensor: (47288, 5, 2)\n"]}],"source":["labels = to_categorical(np.asarray(labels)) # convert to one-hot encoding vectors\n","print('[+] Shape of data tensor:', data.shape)\n","print('[+] Shape of label tensor:', labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdKA167Eo54f"},"outputs":[],"source":["indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIVzFfPmo54f","outputId":"7319bb7f-5f14-4c0f-fb34-4c3a23e70b16"},"outputs":[{"name":"stdout","output_type":"stream","text":["[i] Number of entries in each category:\n","[+] Training:\n"," [[0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]]\n","[+] Validation:\n"," [[1. 0.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n"]}],"source":["x_train = data[:-nb_validation_samples]\n","y_train = labels[:-nb_validation_samples]\n","x_val = data[-nb_validation_samples:]\n","y_val = labels[-nb_validation_samples:]\n","\n","print('[i] Number of entries in each category:')\n","print(\"[+] Training:\\n\",y_train.sum(axis=0))\n","print(\"[+] Validation:\\n\",y_val.sum(axis=0))"]},{"cell_type":"markdown","metadata":{"id":"N-NLkNe7o54f"},"source":["### Preparing the Embedding layer\n","\n","Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings.\n","\n","We use pre-trained [GloVe](https://nlp.stanford.edu/projects/glove/) vectors from Stanford NLP. For new words, a \"randomised vector\" will be created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suwpS1zoo54f","outputId":"83c7d70d-e4d9-498f-a6a8-209537273dd8"},"outputs":[{"ename":"IndexError","evalue":"index 0 is out of bounds for axis 0 with size 0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"]}],"source":["x_train[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wj7x1fSdo54g"},"outputs":[],"source":["embeddings_index = {}\n","f = open(GLOVE_DIR)\n","print(\"[i] Loading GloVe from:\",GLOVE_DIR,\"...\",end=\"\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n","f.close()\n","print(\"Done.\\n[+] Proceeding with Embedding Matrix...\", end=\"\")\n","embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","print(\"[i] Completed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlZM5Mcko54g"},"outputs":[],"source":["print(\"[i] Finished running setup.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}